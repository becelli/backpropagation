{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numba import njit\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vl3oTWgKJQwB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Backpropagation algorithm\n",
        "# default config for the backpropagation algo\n",
        "l_rate = 0.01\n",
        "max_it = 200\n",
        "min_error = 0.1\n",
        "# obs: changing the learning_rate and the initial weight can improve the performance\n",
        "# the current config is the optimal config found for the hyperbolic tangent until this moment\n",
        "\n",
        "# open the csv file and get the data\n",
        "\n",
        "\n",
        "def get_training_data(filename):\n",
        "    with open(filename) as f:\n",
        "        data = pd.read_csv(f)\n",
        "        data = data.values\n",
        "    return data\n",
        "\n",
        "# choose which curve will be used by the neurons of the net\n",
        "\n",
        "# @njit(parallel=True, fastmath=True, cache=True)\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def fx_tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def d_fx_tanh(x):\n",
        "    return (1/np.cosh(x))**2\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def fx_logistic(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def d_fx_logistic(x):\n",
        "    return np.exp(x)/(1+np.exp(x))**2\n",
        "\n",
        "def choose_curve(curve):\n",
        "    if curve == \"tanh\":\n",
        "        return fx_tanh, d_fx_tanh\n",
        "    elif curve == \"logistic\":\n",
        "        return fx_logistic, d_fx_logistic\n",
        "    else:\n",
        "        print(\"OPTION NOT VALID\")\n",
        "        exit()\n",
        "    \n",
        "\n",
        "# initialize the weights as small random values\n",
        "\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def initialize_weights(num_classes: int, num_features: int):\n",
        "    # number of inputs (class features)\n",
        "    num_inputs = num_features\n",
        "    # number of output neurons\n",
        "    num_outputs = num_classes\n",
        "    # number of hidden neurons (geometry mean of inputs and outputs)\n",
        "    num_hidden = int(np.sqrt(num_inputs*num_outputs))\n",
        "    # initialize the weights\n",
        "    # add 0.005 in order to prevent 0\n",
        "    weight_hidden = np.random.rand(num_hidden, num_inputs)/100 + 0.0005\n",
        "    weight_output = np.random.rand(num_outputs, num_hidden) * 3 - 1.5\n",
        "    # initialize the bias, in this case all bias = 0\n",
        "    bias_hidden = np.zeros((num_hidden, num_inputs))\n",
        "    bias_output = np.zeros((num_outputs, num_hidden))\n",
        "    return weight_hidden, weight_output, bias_hidden, bias_output\n",
        "\n",
        "# probably not used anymore..., too lazy to check...\n",
        "# calculate the output for a single neuron with the previous layer output as input\n",
        "# @njit\n",
        "\n",
        "\n",
        "def neuron_output(weights, fx, inputs):\n",
        "    net = np.dot(inputs, weights)\n",
        "    # weighted_input = inputs\n",
        "    # for i in range (0, len(weights)):\n",
        "    #  weighted_input[0] *= weights[0]\n",
        "    # net = weighted_input.sum()\n",
        "    return fx(net)\n",
        "\n",
        "# calculate the output from each neuron of the given layer\n",
        "# each layer is described by set of weights of each neuron\n",
        "\n",
        "# @njit\n",
        "\n",
        "\n",
        "def calculate_layer(weights_layer, fx, layer_inputs):\n",
        "    net = weights_layer @ np.array(layer_inputs)\n",
        "    outputs = []\n",
        "    for i in range(0, len(net)):\n",
        "        outputs.append(fx(net[i]))\n",
        "    return outputs\n",
        "    # return fx(net)\n",
        "\n",
        "# calculate the expected output for the class\n",
        "\n",
        "\n",
        "def calculate_expected_values(curve_type, class_number, num_classes):\n",
        "    expected_values = 0\n",
        "    if curve_type == \"logistic\":\n",
        "        expected_values = np.zeros(num_classes)\n",
        "        expected_values[class_number-1] = 1\n",
        "    elif curve_type == \"tanh\":\n",
        "        expected_values = np.full(num_classes, -1)\n",
        "        expected_values[class_number-1] = 1\n",
        "    else:\n",
        "        exit()\n",
        "    return expected_values\n",
        "\n",
        "# calculate the error for the output layer\n",
        "\n",
        "\n",
        "def calculate_output_error(curve, weights, expected, attained, inputs):\n",
        "    fx, d_fx = choose_curve(curve)\n",
        "    error = []\n",
        "    for i in range(0, len(expected)):\n",
        "        err = (expected[i] - attained[i])*d_fx(inputs[i])\n",
        "        error.append(err)\n",
        "    return error\n",
        "\n",
        "# calculate the error for the hidden layer\n",
        "# needs revision\n",
        "\n",
        "\n",
        "def calculate_hidden_error(curve, weights_hidden, weights_output, error_output, inputs):\n",
        "    fx, d_fx = choose_curve(curve)\n",
        "    error = []\n",
        "    for i in range(0, len(weights_hidden)):\n",
        "        err = np.dot(error_output, weights_output[i])\n",
        "        err *= d_fx(np.dot(inputs, weights_hidden[i]))\n",
        "        error.append(err)\n",
        "    return error\n",
        "\n",
        "# adjust the weights of a given layer according to the layer error\n",
        "\n",
        "\n",
        "def adjust_weights(weights, learning_rate, error, layer_input):\n",
        "    new_weights = []\n",
        "    for i in range(0, len(weights)):\n",
        "        neuron_weights = []\n",
        "        for j in range(0, len(weights[0])):\n",
        "\n",
        "            neuron_weights.append(\n",
        "                weights[i][j] + learning_rate*error[i]*layer_input[j])\n",
        "        new_weights.append(neuron_weights)\n",
        "    return new_weights\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKBwiVbwKjT"
      },
      "source": [
        "Separação das amostras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CSaWoxp_wJyr"
      },
      "outputs": [],
      "source": [
        "# creio que a separacao de amostras seja desnecessaria\n",
        "# algumas observacoes:\n",
        "# a base de treinamento esta desbalanceada\n",
        "# ~1/4 dos dados sao da classe 3 enquanto ~1/4 sao das classes 1 e 2\n",
        "# durante o treinamento de dados, seria interessante tentar balancear os dados\n",
        "# dividindo o conjunto de treino em 5 conjuntos, cada um contendo apenas as\n",
        "# amostras de sua respectiva classe, e permutar ciclicamente cada conjunto.\n",
        "# e.g., treinar com uma amostra do conjunto1 -> treinar com uma amostra do conjunto2 -> conjunto3 -> conjunto4 -> conjunto5 -> conjunto1 -> ...\n",
        "\n",
        "def choose_sample(class_number, data):\n",
        "    sample_size = len(data)\n",
        "    random_sample = random.randint(0, sample_size-1)\n",
        "    sample = data[random_sample]\n",
        "    while (sample[-1] != class_number):\n",
        "        random_sample = random.randint(0, sample_size-1)\n",
        "        sample = data[random_sample]\n",
        "    return sample\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jFC-QSLkbCa0"
      },
      "source": [
        "Treinamento da rede\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "r9ysPZI6bBgt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n#experimental training with a balanced dataset\\ndef train_through_data(curve, data):\\n  #get information from the dataset\\n  num_classes = np.unique(data[:, -1]).size\\n  num_features = data.shape[1] - 1\\n  sample_size = len(data)\\n\\n  #initialize the initial weights\\n  weight_hidden, weight_output, bias_hidden, bias_output = initialize_weights(num_classes, num_features)\\n  #get the function for the choosen curve\\n  fx, d_fx = choose_curve(curve)\\n  \\n  current_class = 1\\n\\n  #train until max_it\\n  for j in range(0, max_it):\\n    #train the net for sample in dataset\\n    for i in range(0, sample_size):\\n      #choose a random sample in order to prevent overfitting\\n      sample = choose_sample(current_class, data)\\n      \\n      #separate the class from the inputs list, i.e., remove the class type from inputs\\n      sample_class = sample[-1]\\n      inputs = np.delete(sample, len(sample)-1, 0)\\n\\n      #calculate the expected value for this set of features\\n      expected_values = calculate_expected_values(curve, sample_class, num_classes)\\n      \\n      #calculate the value for the hidden layer and the output layer\\n      hidden_values = calculate_layer(weight_hidden, fx, inputs)\\n      output_values = calculate_layer(weight_output, fx, hidden_values)\\n      \\n      #calculate the error for each layer\\n      error_output_layer = calculate_output_error(curve, weight_output, expected_values, output_values, hidden_values)\\n      error_hidden_layer = calculate_hidden_error(curve, weight_hidden, weight_output, error_output_layer, inputs)\\n      #print(\"Expected: \")\\n      #print(expected_values)\\n      #print(\"Output: \")\\n      #print(output_values)\\n      \\n      #adjust weights for each layer\\n      weight_output = adjust_weights(weight_output, l_rate, error_output_layer, hidden_values)\\n      weight_hidden = adjust_weights(weight_hidden, l_rate, error_hidden_layer, inputs)\\n\\n      current_class %= num_classes\\n      current_class += 1\\n    if(j % 100 == 0):\\n      print(j)\\n  #return the trained values for each layer\\n  return weight_hidden, weight_output\\n'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# training with an unbalanced dataset\n",
        "# train a neuron net with a specificied curve and data\n",
        "def train_through_data(curve, data):\n",
        "    # get information from the dataset\n",
        "    num_classes = np.unique(data[:, -1]).size\n",
        "    num_features = data.shape[1] - 1\n",
        "    sample_size = data.shape[0]\n",
        "\n",
        "    # remove the last column from the inputs list, i.e., remove the class type from inputs\n",
        "    inputs = np.delete(data, len(data[0])-1, 1)\n",
        "\n",
        "    # initialize the initial weights\n",
        "    weight_hidden, weight_output, bias_hidden, bias_output = initialize_weights(\n",
        "        num_classes, num_features)\n",
        "    # get the function for the choosen curve\n",
        "    fx, d_fx = choose_curve(curve)\n",
        "\n",
        "    # train until max_it\n",
        "    for j in range(0, max_it):\n",
        "        # train the net for sample in dataset\n",
        "        for i in range(0, sample_size):\n",
        "            # choose a random sample in order to prevent overfitting\n",
        "            new_i = random.randint(0, sample_size-1)\n",
        "\n",
        "            # calculate the expected value for this set of features\n",
        "            expected_values = calculate_expected_values(\n",
        "                curve, data[new_i][-1], num_classes)\n",
        "\n",
        "            # calculate the value for the hidden layer and the output layer\n",
        "            hidden_values = calculate_layer(weight_hidden, fx, inputs[new_i])\n",
        "            output_values = calculate_layer(weight_output, fx, hidden_values)\n",
        "\n",
        "            # calculate the error for each layer\n",
        "            error_output_layer = calculate_output_error(\n",
        "                curve, weight_output, expected_values, output_values, hidden_values)\n",
        "            error_hidden_layer = calculate_hidden_error(\n",
        "                curve, weight_hidden, weight_output, error_output_layer, inputs[new_i])\n",
        "            # print(\"Expected: \")\n",
        "            # print(expected_values)\n",
        "            # print(\"Output: \")\n",
        "            # print(output_values)\n",
        "\n",
        "            # adjust weights for each layer\n",
        "            weight_output = adjust_weights(\n",
        "                weight_output, l_rate, error_output_layer, hidden_values)\n",
        "            weight_hidden = adjust_weights(\n",
        "                weight_hidden, l_rate, error_hidden_layer, inputs[new_i])\n",
        "        if (j % 100 == 0):\n",
        "            print(j)\n",
        "    # return the trained values for each layer\n",
        "    return weight_hidden, weight_output\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "#experimental training with a balanced dataset\n",
        "def train_through_data(curve, data):\n",
        "  #get information from the dataset\n",
        "  num_classes = np.unique(data[:, -1]).size\n",
        "  num_features = data.shape[1] - 1\n",
        "  sample_size = len(data)\n",
        "\n",
        "  #initialize the initial weights\n",
        "  weight_hidden, weight_output, bias_hidden, bias_output = initialize_weights(num_classes, num_features)\n",
        "  #get the function for the choosen curve\n",
        "  fx, d_fx = choose_curve(curve)\n",
        "  \n",
        "  current_class = 1\n",
        "\n",
        "  #train until max_it\n",
        "  for j in range(0, max_it):\n",
        "    #train the net for sample in dataset\n",
        "    for i in range(0, sample_size):\n",
        "      #choose a random sample in order to prevent overfitting\n",
        "      sample = choose_sample(current_class, data)\n",
        "      \n",
        "      #separate the class from the inputs list, i.e., remove the class type from inputs\n",
        "      sample_class = sample[-1]\n",
        "      inputs = np.delete(sample, len(sample)-1, 0)\n",
        "\n",
        "      #calculate the expected value for this set of features\n",
        "      expected_values = calculate_expected_values(curve, sample_class, num_classes)\n",
        "      \n",
        "      #calculate the value for the hidden layer and the output layer\n",
        "      hidden_values = calculate_layer(weight_hidden, fx, inputs)\n",
        "      output_values = calculate_layer(weight_output, fx, hidden_values)\n",
        "      \n",
        "      #calculate the error for each layer\n",
        "      error_output_layer = calculate_output_error(curve, weight_output, expected_values, output_values, hidden_values)\n",
        "      error_hidden_layer = calculate_hidden_error(curve, weight_hidden, weight_output, error_output_layer, inputs)\n",
        "      #print(\"Expected: \")\n",
        "      #print(expected_values)\n",
        "      #print(\"Output: \")\n",
        "      #print(output_values)\n",
        "      \n",
        "      #adjust weights for each layer\n",
        "      weight_output = adjust_weights(weight_output, l_rate, error_output_layer, hidden_values)\n",
        "      weight_hidden = adjust_weights(weight_hidden, l_rate, error_hidden_layer, inputs)\n",
        "\n",
        "      current_class %= num_classes\n",
        "      current_class += 1\n",
        "    if(j % 100 == 0):\n",
        "      print(j)\n",
        "  #return the trained values for each layer\n",
        "  return weight_hidden, weight_output\n",
        "'''\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XE2s_3ZWho05"
      },
      "source": [
        "Testagem da rede\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "K6B6S_yvhoTR"
      },
      "outputs": [],
      "source": [
        "# the assigned class will be the index with the higher value\n",
        "def assign_class(output):\n",
        "    return output.index(max(output)) + 1\n",
        "\n",
        "# create a confusion matrix\n",
        "\n",
        "\n",
        "# def confusion_matrix():\n",
        "#     return\n",
        "\n",
        "\n",
        "def test_network(curve, weight_hidden, weight_output, data):\n",
        "    # get information from the dataset\n",
        "    num_classes = np.unique(data[:, -1]).size\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    num_features = data.shape[1] - 1\n",
        "    sample_size = data.shape[0]\n",
        "    # remove the last column from the inputs list, i.e., remove the class type from inputs\n",
        "    inputs = np.delete(data, len(data[0])-1, 1)\n",
        "\n",
        "    fx, d_fx = choose_curve(curve)\n",
        "    assigned_class = []\n",
        "    true_class = []\n",
        "    count_errors = 0\n",
        "    for i in range(0, sample_size):\n",
        "        # calculate the value for the hidden layer and the output layer\n",
        "        hidden_values = calculate_layer(weight_hidden, fx, inputs[i])\n",
        "        output_values = calculate_layer(weight_output, fx, hidden_values)\n",
        "\n",
        "        # debug output\n",
        "        assigned_class.append(assign_class(output_values))\n",
        "        true_class.append(data[i][-1])\n",
        "        if (true_class[i] != assigned_class[i]):\n",
        "            count_errors += 1\n",
        "        # print(\"Expected: \", true_class[i], \" Attained: \", assigned_class[i])\n",
        "\n",
        "        # debug output\n",
        "        expected_values = calculate_expected_values(\n",
        "            curve, data[i][-1], num_classes)\n",
        "        # print(\"Expected: \")\n",
        "        # print(expected_values)\n",
        "        # print(\"Output: \")\n",
        "        # print(output_values)\n",
        "        confusion_matrix[true_class[i]-1][assigned_class[i]-1] += 1\n",
        "\n",
        "    print('Matriz de confusão')\n",
        "    print(confusion_matrix)\n",
        "\n",
        "    # debug output\n",
        "    print(\"Erros: \")\n",
        "    print(count_errors)\n",
        "    print(\"Total de amostras: \")\n",
        "    print(sample_size)\n",
        "    # print(\"Peso oculta: \")\n",
        "    # print(weight_hidden)\n",
        "    # print(\"Peso saida: \")\n",
        "    # print(weight_output)\n",
        "    return\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pABtZ55klCFw"
      },
      "source": [
        "Programa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SS84qE09lBPZ"
      },
      "outputs": [],
      "source": [
        "def backpropagation_algo():\n",
        "    # curve = \"tanh\"\n",
        "    curve = \"logistic\"\n",
        "    data = get_training_data(\"treinamento.csv\")\n",
        "    weight_hidden, weight_output = train_through_data(curve, data)\n",
        "\n",
        "    data = get_training_data(\"teste.csv\")\n",
        "    test_network(curve, weight_hidden, weight_output, data)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWEYhaW9Yuhv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkCmwncLV5hs",
        "outputId": "cfd1edba-b8c1-49d2-8b8d-0e879cde9470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "Matriz de confusão\n",
            "[[ 36.   0.  20.   0.   0.]\n",
            " [  0.   0.   0.  53.   0.]\n",
            " [  0.   0. 102.   0.   0.]\n",
            " [  0.   0.   0.  75.   0.]\n",
            " [  0.   0.   0.   0.  66.]]\n",
            "Erros: \n",
            "73\n",
            "Total de amostras: \n",
            "352\n"
          ]
        }
      ],
      "source": [
        "backpropagation_algo()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ea9437200182d95efa26cebbb8d9c138f7faf70d27801ebcabc0dc06f6872427"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
